---
title: "Calculate hash proportions within samples"
author: "Joe Duprey, Helen Casendino, Ramon Gallego"
date: "10/26/2021"
output: html_document
---

# Overview
This code creates the following four csv files: 
  1) long_PCR_props.csv. This code calculates the proportions for each hash within all PCR reps. 
  2) long_trip_PCR_props.csv. This code calculates the proportions for each hash within a PCR triplicate without removing PCR replicates with low reads.
  3) clean_trip_PCR_props.csv. This code calculates the proportions for each hash within a PCR triplicate AFTER removing PCR replicates with low reads.
  4) clean_dup_PCR_props.csv. This code calculates the proportions for each hash within a PCR duplicate.
  
```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r dependencies}
library(tidyverse)
```

## STEP 1: load raw ASV data. 

```{r load data and bring it into standard form}
raw_ASVs <- read_csv("../input/raw_ASVs.csv") # ADD NOTES ABOUT REQUIRED FORMAT FOR INPUT

all_data <- raw_ASVs %>%
  rename(seq_run = Miseq_run, hash = Hash, reads = nReads) %>% # for consistent lowercase names
  filter(str_detect(sample, "Ostrich", TRUE)) %>% # remove ostrich samples
  filter(str_detect(sample, "Kangaroo", TRUE)) %>% # remove kangaroo samples
  filter(str_detect(sample, "K+", TRUE)) %>% # remove negative control (??) samples
  filter(str_detect(sample, "k+", TRUE)) %>%
  mutate(sample = gsub("_", "", sample)) %>% # for consistency across sample names
  separate(col = sample, into = c("sample", "tech"), sep = "[.]", remove = FALSE) %>% 
  mutate(tech = as.integer(tech)) %>% 
  mutate(site = str_sub(sample, 1, -2)) %>% 
  mutate(bio = str_sub(sample, -1, -1)) %>% 
  dplyr::select(seq_run, site, bio, tech, hash, reads)

# Check for repeats
repeats <- all_data %>%
  group_by(site, bio, tech, hash, reads) %>%
  filter(n() > 1)
if(nrow(repeats) > 0) warning('all_data has repeats!')

write_csv(all_data, "../data/all_data.csv")
```

## STEP 2: Create csv of proportional data (all replicates) . 

```{r creating long_PCR_props.csv}
PCR_props <- all_data %>%
  group_by(site, bio, tech) %>%
  mutate(prop = reads / sum(reads))

write_csv(PCR_props, "../data/long_PCR_props.csv")
```

## STEP 3: Create csv of triplicate PCR proportional data (no low reads removed)

```{r filter in triplicates}
triplicates <- all_data %>%
  group_by(bio) %>%
  summarise(nrep = n_distinct(bio, tech)) %>%
  filter(nrep == 3)
```

```{r getting hash proportions, creating long_trip_PCR_props.csv}
long_trip_PCR_props <- all_data %>%
  filter(bio %in% triplicates$bio) %>% # only take bio reps with three PCR reps
  group_by(bio, tech) %>%
  mutate(prop = reads / sum(reads)) %>%
  dplyr::select(bio, tech, hash, prop)

write_csv(long_trip_PCR_props, "../data/long_trip_PCR_props.csv")
```

## STEP 4: Create csv of triplicate proportional data (low reads removed)

Data frame that excludes very low reads within PCR triplicates. We'll adapt our code from Step 2 of Moncho's (Denoising.all.runs.Rmd) in the OA paper, where reads are fit to a normal distribution and those with low reads outside of the 95% CI are removed. 

```{r fit reads to normal dist}

triplicate.data <- all_data %>%
  filter(bio %in% triplicates$bio)

all.reads.sums <- triplicate.data %>%
  group_by(sample) %>%
  summarise(tot = sum(reads)) # getting sum of reads for each PCR replicate

# visualize total read distribution
hist(all.reads.sums$tot, breaks = 80)

reads.per.sample <- all.reads.sums %>%
  pull(tot)

names(reads.per.sample) <- all.reads.sums %>% pull(sample)

normparams.reads <- MASS::fitdistr(reads.per.sample, "normal")$estimate

#### r removing reads outside of 95% interval

all.reads.sums <- all.reads.sums %>%
  mutate(prob = pnorm(tot, normparams.reads[1], normparams.reads[2]))

outliers <- all.reads.sums %>%
  filter(prob < 0.075 & tot < normparams.reads[1])
```

We'll make the triplicate proportion table here.

```{r Making triplicate prop table called clean_trip_PCR_props.csv}
PCR_props.clean <- triplicate.data %>% 
  filter(!sample %in% outliers$sample) %>%
  group_by(bio, tech) %>% # removed seq_run 8/27 see above - joe
  mutate(prop = reads / sum(reads)) %>%
  dplyr::select(seq_run, bio, tech, hash, prop)

write_csv(PCR_props.clean, "../data/clean_trip_PCR_props.csv")
```

## STEP 5: Create csv of duplicate proportional data (low reads removed)

Now, we also need to repeat with our duplicates! As a reminder, we also need to check for low reads from these. 

```{r isolate duplicates}
# Getting true duplicates
duplicates <- all_data %>%
  group_by(bio) %>%
  summarise(nrep = n_distinct(bio, tech)) %>%
  filter(nrep == 2)
```

Now we're gonna check, just to make sure, that no reads from the true duplicates are really low. 

```{r checking to make sure no dups have really low reads}
duplicate_data <- all_data %>%
  filter(bio %in% duplicates$bio) 

all.reads.sums <- duplicate_data %>%  group_by(sample) %>%
  summarise(tot = sum(reads))

 reads.per.sample <- all.reads.sums %>%
  pull(tot)

 names(reads.per.sample) <- all.reads.sums %>% pull(sample)

 normparams.reads <- MASS::fitdistr(reads.per.sample, "normal")$estimate

 all.reads.sums <- all.reads.sums %>%
 mutate(prob = pnorm(tot, normparams.reads[1], normparams.reads[2]))

 all.reads.sums %>%
  filter(prob < 0.075 & tot < normparams.reads[1])
```

Nothing too low! So, let's make a csv with duplicate proportions.

```{r duplicate proportions, called clean_dup_PCR_props.csv}
PCR.duplic.proportions <- duplicate_data %>%
  group_by(bio, tech) %>% 
  mutate(prop = reads / sum(reads)) %>%
  dplyr::select(seq_run, bio, tech, hash, prop) 

write_csv(PCR.duplic.proportions, "../data/clean_dup_PCR_props.csv")
```
