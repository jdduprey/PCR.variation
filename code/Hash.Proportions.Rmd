---
title: "Calculate Hash proportions within samples (& triplicate eDNA index)"
author: "Joe Duprey, Helen Casendino, Ramon Gallego"
date: "8/25/2021"
output: html_document
---

# Overview
This code creates the following four csv files: 
  1) PCR.proportions.csv. This code calculates the proportions for each Hash within a PCR triplicate without removing PCR replicates with low reads.
  2) PCR.proportions.clean.csv. This code calculates the proportions for each Hash within a PCR triplicate AFTER removing PCR replicates with low reads.
  3) PCR.duplic.proportions.csv. This code calculates the proportions for each Hash within a PCR duplicate. These duplicate include true duplicates AND samples that had one PCR replicate removed in the earlier low read quality control step. 
  4) PCR.index.clean.csv. This code calculates the eDNA index for each Hash within a PCR triplicate.
  
  **NOTE: For PCR.proportions.clean.csv and PCR.duplic.proportions.csv, Hash proportions are averaged across duplicate MiSeq runs, where the same bottles and corresponding Hashes were run through two Miseq runs (I believe Miseq runs 5 & 6 have duplicates)**
  
```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r Dependencies}
library(tidyverse)
```

First get raw ASV data. 

```{r Load data and create technical and biological replicate columns}
raw.ASVs <- read.csv('../input/raw_ASVs.csv')  # ADD NOTES ABOUT REQUIRED FORMAT FOR INPUT

by.tech.table <- raw.ASVs %>%
  filter(str_detect(sample, "Ostrich", TRUE)) %>%  # remove ostrich samples
  filter(str_detect(sample, "Kangaroo", TRUE)) %>%  # remove kangaroo samples
  filter(str_detect(sample, "K+", TRUE)) %>%  # remove negative control (??) samples
  filter(str_detect(sample, "k+", TRUE)) %>% 
  mutate(sample = gsub("_", "", by.tech.table$sample)) %>%  # for consistency across sample names
  separate(col = sample, into = c('bio', 'tech'), sep = '[.]', remove = FALSE) %>% # WHY DON'T WE REMOVE?
  group_by(sample, bio, tech, Hash, nReads) %>%  # eliminate duplicates
  filter(n() == 1)  # THIS SHOULD BE EQUALS, RIGHT?  
```

## Triplicate proportions with low reads included

For now, only use samples with 3 PCR replicates. 

```{r Filter in triplicates}
three.reps <- by.tech.table %>%
  group_by(bio) %>%
  summarise(nrep = n_distinct(bio, tech)) %>%
  filter(nrep == 3)
```

Now, we want to get the proportion of nReads for each hash (nReads of hash/total nReads in the bio sample for each tech rep). 

```{r Getting Hash Proportions}
PCR.proportions <- by.tech.table %>%
  filter(bio %in% three.reps$bio) %>% # only take bio reps with three PCR reps
  group_by(bio, tech) %>% # removed Miseq_run from group_by() i think this will solve our duplicate hash problem -joe   
  mutate(PropRead = nReads/sum(nReads)) %>%
  select(bio, tech, Hash, PropRead) %>% # 10/13/2021 MISEQ RUN - ASK HELEN/RYAN  
  pivot_wider(names_from=tech, values_from=PropRead, values_fill=0) %>% 
  rename(c("PCR1_prop" = "1", "PCR2_prop" = "2", "PCR3_prop" = "3"))

# 10/14/2021 We're back to the miseq issue - lets check that proportions sum to 1 - 
```

Save the new dataframe. 

```{r include df in data}
write_csv(PCR.proportions, '../data/PCR.proportions.csv')
```



## Triplicate proportions with low reads removed

Data frame that excludes very low reads within PCR triplicates. We'll adapt our code from Step 2 of Moncho's (Denoising.all.runs.Rmd) in the OA paper, where nReads are fit to a normal distribution and those with low reads outside of the 95% CI are removed. 

```{r Fit nReads to normal dist}

triplicate.data.allreads<- by.tech.table %>%
  filter(bio %in% three.reps$bio)

all.nReads.sums <- triplicate.data.allreads %>%  
  group_by(sample) %>%
  summarise(tot = sum(nReads)) # getting sum of reads for each PCR replicate

# visualize total read distribution 
hist(all.nReads.sums$tot, breaks=80)

reads.per.sample <- all.nReads.sums %>%  
  pull(tot)

names(reads.per.sample) <- all.nReads.sums %>% pull(sample)  

normparams.reads <- MASS::fitdistr(reads.per.sample, "normal")$estimate
```

Now we'll get the PCR replicates with reads below the 95% interval and remove those from the data set. We'll create a revise proportions csv. 

```{r Removing reads outside of 95% interval}
all.nReads.sums <- all.nReads.sums %>%  
  mutate(prob = pnorm(tot, normparams.reads[1], normparams.reads[2]))

outliers <- 
  all.nReads.sums %>% 
  filter(prob < 0.075 & tot < normparams.reads[1])
```

So taking a break here - we now have biological replicates that need one PCR replicate removed because it's read was too low. So in essence, these samples are now duplicates. We can combine these samples with our true duplicates to make a triplicate and a duplicate table. 

```{r Isolate duplicates}
# Getting true duplicates
two_reps <- by.tech.table %>%
  group_by(bio) %>%
  summarise(nrep = n_distinct(bio, tech)) %>%
  filter(nrep == 2)

# biological sample identifiers of outliers from triplicate (low reads, now duplicates). Add these two two_reps
outliers$bio<- gsub('..$', '', outliers$sample)
two_reps_full <- two_reps %>% rows_insert(tibble(bio = outliers$bio, nrep = 2))
```

Now remove the new duplicate samples from the triplicate set.
We'll make the triplicate proportion table here. We also don't want duplicates across Miseq runs (the same bio.Hash row for Miseq runs 5 & 6), so we will average proportion across Miseq runs. 

```{r removing duplicates from triplicate & making triplicate prop table}

true.triplicate.data<- by.tech.table %>%
  filter(!bio %in% two_reps_full$bio)

PCR.proportions.clean <- true.triplicate.data %>%
  group_by(bio, tech) %>% # removed Miseq_run 8/27 see above - joe 
  mutate(PropRead = nReads/sum(nReads)) %>%
  select(bio, tech, Hash, PropRead) %>%
  pivot_wider(names_from=tech, values_from=PropRead, values_fill=0) %>% 
  dplyr::rename(c('PCR1_prop' = '1', 'PCR2_prop' = '2', 'PCR3_prop' = '3'))%>%    group_by(bio, Hash)%>% 
  summarise_at(c("PCR1_prop", "PCR2_prop", "PCR3_prop"), mean)

write_csv(PCR.proportions.clean, '../data/PCR.proportions.clean.csv')
# something like this to get hash proportions between bio replicates 
# PCR.by.bio.clean <- triplicate.data.clean %>%
#   group_by(Miseq_run, bio, tech) %>%
#   mutate(PropRead = nReads/sum(nReads)) %>%
#   select(Miseq_run, bio, tech, Hash, PropRead) %>%
#   pivot_wider(names_from=bio, values_from=PropRead, values_fill=0) #%>% 
  # rename(c("PCR1_prop" = "1", "PCR2_prop" = "2", "PCR3_prop" = "3"))

#write_csv(PCR.by.bio.clean, '../data/PCR.proportions.clean.csv')

```



## Duplicate proportions 

Now, we also need to repeat with our duplicates! As a reminder, we also need to check for low reads from these. 

```{r creating full duplicate df}

duplicate.data<- by.tech.table %>%
  filter(bio %in% two_reps_full$bio) # both low read and true duplicates

# remove PCR *replicates* that had low reads (so leave the remaining two for that bio sample)
duplicate.data.clean <- duplicate.data %>% filter(!sample %in% outliers$sample)
```

Now we're gonna check, just to make sure, that no reads from the true duplicates are really low. 

```{r checking to make sure no dups have really low reads}
#all.nReads.sums <- duplicate.data.clean %>%  group_by(sample) %>%
#  summarise(tot = sum(nReads)) 

#reads.per.sample <- all.nReads.sums %>%  
#  pull(tot)

#names(reads.per.sample) <- all.nReads.sums %>% pull(sample)  

#normparams.reads <- MASS::fitdistr(reads.per.sample, "normal")$estimate

#all.nReads.sums <- all.nReads.sums %>%  
#  mutate(prob = pnorm(tot, normparams.reads[1], normparams.reads[2]))

#all.nReads.sums %>% 
#  filter(prob < 0.075 & tot < normparams.reads[1])
```

Nothing too low! So, let's make a csv with duplicate proportions. But ew, some samples have PCR 1&2, others have PCR 2&3, etc. So let's change all to PCR 1&2. 

```{r duplicate data table clean up}
# bioVector is list of unique bio sample IDs, tech.col is the column of the df that lists tech rep
standard_PCRnums <- function(dup.df, bioVector, tech.col){
  
  for(i in 1:length(bioVector)){
      allPCR.nums <- dup.df[dup.df$bio == bioVector[i],tech.col] # part of dup.df for 1 bio sample that lists all tech rep numbers
      uniquePCR.nums <- unique(dup.df[dup.df$bio == bioVector[i],tech.col]) # what are the 2 numbers of the PCR reactions done? ie, 1 & 2, 2 & 3, etc.
     
       rows1 <-which(allPCR.nums == uniquePCR.nums[1]) # which rows equal the 1st PCR rep 
     rows2 <-which(allPCR.nums == uniquePCR.nums[2])
     
     allPCR.nums[rows1] <- "1"  # convert the first PCR number to 1 within the tech column of the loop's dataframe
     allPCR.nums[rows2] <- "2"
      dup.df[dup.df$bio == bioVector[i],tech.col] <- allPCR.nums # put this new number string into dup.df
  }
  return(dup.df)
} 

standard.props <- standard_PCRnums(duplicate.data.clean, unique(duplicate.data.clean$bio), 4)
```


```{r duplicate proportions}

# calculate duplicate proportions
PCR.duplic.proportions <- standard.props %>%
  group_by(bio, tech) %>% # removed Miseq_run 8/27, see above 
  mutate(PropRead = nReads/sum(nReads)) %>%
  select(bio, tech, Hash, PropRead) %>% 
  pivot_wider(names_from=tech, values_from=PropRead, values_fill = 0) %>% 
  dplyr::rename(c("PCR1_prop" = "1", "PCR2_prop" = "2")) %>% group_by(bio, Hash)%>%  summarise_at(c("PCR1_prop", "PCR2_prop"), mean)

write_csv(PCR.duplic.proportions, '../data/PCR.duplic.proportions.csv')
```

## Triplicate eDNA INDICES with low reads removed

I also want to create a csv with eDNA index values instead of proportions. 

```{r Sourcing eDNA index func}
# source("/Users/helencasendino/Desktop/PCR.variation/code/eDNA indexing/helen.eDNA_index.R") # modified index function (need to fix source path)
calc_eDNA_index <- function(df, PCRcols) { 
  
  for(i in 1:length(PCRcols)){
    
    nRead.col <- colnames(df)[PCRcols[i]] # col name of PCR column
    
    df[,nRead.col] <- df %>%
      group_by(bio) %>% # "convert read counts to proportion with biological sample" 
      mutate(totalReads = sum(.data[[nRead.col]]),
             propReads = .data[[nRead.col]]/totalReads) %>% 
      
      group_by(Hash) %>% # "scale the resulting proportion to the largest observed proportion across samples"
      mutate(index = propReads/(max(propReads)), .keep = "none") %>% ungroup() %>% select(-Hash)
    
    NArows <- which(is.na(df[,nRead.col]) == T) 
    df[NArows,nRead.col] <- 0
  }
  colnames(df)[PCRcols] <- c("PCR1_index", "PCR2_index", "PCR3_index") # rename columns to "index"
  return(df)
} 
```

```{r run function on trip data}
# Make trip data wide (1 column per PCR)
wide <- true.triplicate.data %>%  
  select(bio, tech, Hash, nReads) %>%
  pivot_wider(names_from=tech, values_from=nReads, values_fill=0)

wide.named <- wide %>%
  rename('PCR1_reads'='1', 'PCR2_reads'='2', 'PCR3_reads'='3')

PCR.index.clean <- calc_eDNA_index(wide.named, c(3:5))
write_csv(PCR.index.clean, '../data/PCR.index.clean.csv')
```

#####################################################################################